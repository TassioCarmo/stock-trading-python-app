# Stock Data Pipeline ğŸš€

Um sistema profissional de pipeline de dados para coleta, processamento e armazenamento de informaÃ§Ãµes financeiras em tempo real. Desenvolvido para demonstrar habilidades em engenharia de dados, APIs, e integraÃ§Ã£o com cloud services.

## ğŸ¯ Sobre o Projeto

Este projeto implementa um pipeline completo de dados financeiros que coleta informaÃ§Ãµes de aÃ§Ãµes da Polygon API, oferecendo mÃºltiplas opÃ§Ãµes de armazenamento e execuÃ§Ã£o automatizada. Ideal para demonstraÃ§Ã£o de habilidades em desenvolvimento backend e engenharia de dados.

## âœ¨ Destaques TÃ©cnicos

### ğŸ—ï¸ Arquitetura e Design
- **Arquitetura modular** com separaÃ§Ã£o de responsabilidades
- **PadrÃ£o de resiliÃªncia** com sistema de resume automÃ¡tico
- **ConfiguraÃ§Ã£o externalizada** via environment variables
- **Logging detalhado** para monitoramento do pipeline

### ğŸ”§ Habilidades Demonstradas
- **IntegraÃ§Ã£o com APIs REST** (Polygon.io) com rate limiting
- **Processamento de dados** com Pandas e manipulaÃ§Ã£o de JSON
- **Banco de dados cloud** (Snowflake) com conexÃ£o otimizada
- **Agendamento de tarefas** com schedule e controle de execuÃ§Ã£o
- **GestÃ£o de dependÃªncias** e ambiente virtual Python
- **Tratamento de erros** e exceÃ§Ãµes robusto
- **Versionamento de dados** com timestamps e metadados

### ğŸ“Š Funcionalidades AvanÃ§adas
- **Resume inteligente** - Continua de interrupÃ§Ãµes sem perda de dados
- **Rate limiting adaptativo** - Respeita limites da API automaticamente
- **MÃºltiplos destinos** - CSV local ou Snowflake na nuvem
- **Backup progressivo** - Checkpoints a cada pÃ¡gina de dados
- **Cleanup automÃ¡tico** - GestÃ£o de arquivos temporÃ¡rios

## ğŸ› ï¸ Stack TecnolÃ³gico

| Camada | Tecnologias |
|--------|-------------|
| **Linguagem** | Python 3.7+ |
| **APIs** | Polygon.io REST API |
| **Banco de Dados** | Snowflake (Cloud) |
| **Data Processing** | Pandas, JSON |
| **Scheduling** | schedule, time |
| **Environment** | python-dotenv, os |
| **HTTP Requests** | requests |

## ğŸš€ ComeÃ§ando

### PrÃ©-requisitos
- Python 3.7 ou superior
- Chave de API gratuita do [Polygon.io](https://polygon.io/)
- (Opcional) Conta Snowflake para demonstraÃ§Ã£o cloud

### InstalaÃ§Ã£o RÃ¡pida

1. **Clone e prepare o ambiente**
```bash
git clone https://github.com/seu-usuario/stock-data-pipeline.git
cd stock-data-pipeline
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows
```

2. **Instale as dependÃªncias**
```bash
pip install -r requirements.txt
```

3. **Configure as variÃ¡veis de ambiente**
```bash
cp .env.example .env
# Edite .env com suas chaves API
```

## âš™ï¸ ConfiguraÃ§Ã£o

### Polygon API (Gratuita)
1. Registre-se em [polygon.io](https://polygon.io/)
2. Obtenha sua API key gratuita
3. Adicione ao `.env`:
```env
POLYGON_API_KEY=sua_chave_aqui
```

### Snowflake (Opcional - DemonstraÃ§Ã£o Cloud)
```env
SNOWFLAKE_ACCOUNT=seu_account
SNOWFLAKE_USER=seu_usuario
SNOWFLAKE_PASSWORD=sua_senha
SNOWFLAKE_WAREHOUSE=COMPUTE_WH
SNOWFLAKE_DATABASE=STOCKS_DB
SNOWFLAKE_SCHEMA=PUBLIC
SNOWFLAKE_ROLE=ACCOUNTADMIN
```

## ğŸ’» Uso

### Modo de ExecuÃ§Ã£o Ãšnica
```bash
# Para CSV local
python script-csv.py

# Para Snowflake (cloud)
python script-snowflake.py
```

### Modo Agendado (Production-like)
```bash
python scheduler.py
```

## ğŸ“ˆ Estrutura de Dados

### Schema dos Tickers
```python
COLUMNS = [
    "ticker",           # SÃ­mbolo (AAPL, TSLA)
    "name",             # Nome da empresa
    "market",           # Tipo de mercado
    "locale",           # RegiÃ£o geogrÃ¡fica
    "primary_exchange", # Bolsa principal
    "type",             # Tipo de seguranÃ§a
    "active",           # Status ativo/inativo
    "currency_name",    # Moeda de negociaÃ§Ã£o
    "cik",              # SEC identifier
    "composite_figi",   # FIGI global
    "share_class_figi", # FIGI da classe
    "last_updated_utc"  # Timestamp UTC
]
```

## ğŸ—ï¸ Estrutura do Projeto

```
stock-data-pipeline/
â”œâ”€â”€ scheduler.py              # ğŸ•’ Agendador de execuÃ§Ãµes
â”œâ”€â”€ script-csv.py             # ğŸ’¾ Coleta para CSV local
â”œâ”€â”€ script-snowflake.py       # â˜ï¸  Coleta para Snowflake
â”œâ”€â”€ requirements.txt          # ğŸ“¦ DependÃªncias
â”œâ”€â”€ .env.example             # ğŸ” Template variÃ¡veis ambiente
â””â”€â”€ README.md               # ğŸ“š DocumentaÃ§Ã£o
```

## ğŸ”„ Fluxo de Dados

1. **ConexÃ£o API** â†’ AutenticaÃ§Ã£o e rate limiting
2. **PaginaÃ§Ã£o** â†’ Coleta incremental com resume
3. **TransformaÃ§Ã£o** â†’ Cleanup e padronizaÃ§Ã£o
4. **PersistÃªncia** â†’ CSV ou Snowflake
5. **Logging** â†’ Monitoramento e debug

## ğŸ¯ Casos de Uso Demonstrados

### ğŸ’¼ Para Entrevistas TÃ©cnicas
- **Engenharia de Dados**: Pipeline completo ETL/ELT
- **Backend Development**: IntegraÃ§Ã£o API + Database
- **DevOps**: Scheduling + Environment management
- **Data Engineering**: Cloud data warehousing

### ğŸ¢ AplicaÃ§Ãµes Reais
- **Financial Analytics**: Base para anÃ¡lise de aÃ§Ãµes
- **Data Products**: Fonte para aplicaÃ§Ãµes financeiras
- **ML Pipelines**: Dados para modelos preditivos
- **Reporting**: Dados para dashboards e relatÃ³rios

## ğŸ” Exemplo de CÃ³digo

### Sistema de Resume Inteligente
```python
def save_progress(next_url, tickers):
    """Salva progresso para resume em caso de falhas"""
    progress = {"next_url": next_url, "ticker_count": len(tickers)}
    with open("progress.json", 'w') as f:
        json.dump(progress, f)
    # Backup incremental dos dados
    if tickers:
        pd.DataFrame(tickers).to_csv("tickers_partial.csv", index=False)
```

### ConexÃ£o Cloud Optimizada
```python
def upload_to_snowflake(df):
    """Upload otimizado para Snowflake com gestÃ£o de recursos"""
    conn = snowflake.connector.connect(
        user=os.getenv("SNOWFLAKE_USER"),
        password=os.getenv("SNOWFLAKE_PASSWORD"),
        account=os.getenv("SNOWFLAKE_ACCOUNT"),
        # ... configuraÃ§Ãµes
    )
    # Upload batch eficiente
    success, nchunks, nrows, _ = write_pandas(conn, df, "STOCK_TICKERS")
```

## ğŸ“Š MÃ©tricas do Sistema

- **â‰ˆ8,000+ tickers** coletados por execuÃ§Ã£o
- **â‰ˆ2-3 horas** para coleta completa (rate limit)
- **100% resiliÃªncia** a interrupÃ§Ãµes
- **Dual storage** local e cloud

## ğŸš¨ SoluÃ§Ã£o de Problemas

### Erros Comuns
```bash
# Rate Limit Exceeded
"Waiting 12 seconds before next request..."

# API Key InvÃ¡lida
"Verifique POLYGON_API_KEY no .env"

# Snowflake Connection
"Confirme variÃ¡veis de ambiente do Snowflake"
```

### Debug
```python
# Ative logging detalhado
import logging
logging.basicConfig(level=logging.DEBUG)
```



## ğŸ“„ LicenÃ§a

DistribuÃ­do sob licenÃ§a MIT. Veja `LICENSE` para mais informaÃ§Ãµes.

## ğŸ‘¨ğŸ’» Autor

Tassio Carmo- [GitHub](https://github.com/TassioCarmo) - [LinkedIn](https://linkedin.com/in/tassioluiz)
